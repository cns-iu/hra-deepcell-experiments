{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18909d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tifffile import TiffFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b9ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_root = \"/teradata/sbdubey/deepcell-experiments-data/intestine-codex-stanford/data-original/\"\n",
    "output_root = \"/teradata/sbdubey/deepcell-experiments-data/intestine-codex-stanford/input-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406e6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hubmap_id(dir_name):\n",
    "    # Extract HBM###.XXXX.### part and remove dots\n",
    "    parts = dir_name.split('-')[0]\n",
    "    return parts.replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645845e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mpp(tiff_path):\n",
    "    with TiffFile(tiff_path) as tif:\n",
    "        ome_xml = tif.ome_metadata\n",
    "        \n",
    "        # Parse PhysicalSizeX and unit\n",
    "        if 'PhysicalSizeX=\"' in ome_xml:\n",
    "            start = ome_xml.find('PhysicalSizeX=\"') + 15\n",
    "            end = ome_xml.find('\"', start)\n",
    "            physical_size_x = float(ome_xml[start:end])\n",
    "            \n",
    "            start = ome_xml.find('PhysicalSizeXUnit=\"') + 19\n",
    "            end = ome_xml.find('\"', start)\n",
    "            unit = ome_xml[start:end]\n",
    "            \n",
    "            # Convert to microns\n",
    "            if unit == 'nm':\n",
    "                mpp = physical_size_x / 1000\n",
    "            elif unit == 'Âµm' or unit == 'um':\n",
    "                mpp = physical_size_x\n",
    "            elif unit == 'mm':\n",
    "                mpp = physical_size_x * 1000\n",
    "            elif unit == 'm':\n",
    "                mpp = physical_size_x * 1000000\n",
    "            else:\n",
    "                mpp = physical_size_x\n",
    "                \n",
    "            return round(mpp, 5)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232a07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yaml_config(json_path, tiff_path, hubmap_id):\n",
    "    with open(json_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Extract channel names\n",
    "    channel_names = config['channel_names']\n",
    "    \n",
    "    # Extract nucleus and cell channels\n",
    "    nucleus_channel = config['report']['reg1']['nucleus_channel']\n",
    "    cell_channel = config['report']['reg1']['cell_channel']\n",
    "    \n",
    "    # Get channel indices\n",
    "    nucleus_idx = channel_names.index(nucleus_channel)\n",
    "    cell_idx = channel_names.index(cell_channel)\n",
    "    \n",
    "    # Extract MPP\n",
    "    mpp = extract_mpp(tiff_path)\n",
    "    \n",
    "    # Build YAML structure\n",
    "    yaml_data = {\n",
    "        'image_path': f'{hubmap_id}.ome.tiff',\n",
    "        'use_wsi': True,\n",
    "        'MPP': mpp,\n",
    "        'channels': [\n",
    "            {'name': nucleus_channel, 'number': nucleus_idx},\n",
    "            {'name': cell_channel, 'number': cell_idx}\n",
    "        ],\n",
    "        'markers': [\n",
    "            {'name': name, 'number': idx} \n",
    "            for idx, name in enumerate(channel_names)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return yaml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: HBM233.GTZN.466-b38730b2633e0b088619f9bcd514ba13\n",
      "  Created: HBM233GTZN466\n",
      "Processing: HBM244.TVNH.734-168050d20802e0c0d91fd5f51ce550da\n",
      "  Created: HBM244TVNH734\n",
      "Processing: HBM245.NHMB.685-bc68fe67a089ab19c1449de6d0703d71\n",
      "  Created: HBM245NHMB685\n",
      "Processing: HBM253.MXKW.373-ac169bbda02d0c2832c01f70375ff6dc\n",
      "  Created: HBM253MXKW373\n",
      "Processing: HBM292.FCMS.497-6bdd149dc47782aefdd0e23599708183\n",
      "  Created: HBM292FCMS497\n",
      "Processing: HBM293.LGZW.236-c92332aa7e244be5bad1c27c80fcd343\n",
      "  Created: HBM293LGZW236\n",
      "Processing: HBM334.RPTP.997-47b8410d1c51b23e7fb1a721c53a493f\n",
      "  Created: HBM334RPTP997\n",
      "Processing: HBM352.MDZF.598-01510a4fb90fd303bd48c4cd51cdd14c\n",
      "  Created: HBM352MDZF598\n",
      "Processing: HBM396.FNQW.543-87922a42fa8bc7ab29a4d2d5374afbb4\n",
      "  Created: HBM396FNQW543\n",
      "Processing: HBM398.SWKV.256-ff77fcae7f6d9b5b7b8741c282677eef\n",
      "  Created: HBM398SWKV256\n",
      "Processing: HBM423.MMGW.744-3e800f0cd138b989b935fb94e7938617\n",
      "  Created: HBM423MMGW744\n",
      "Processing: HBM423.QJJR.545-b98ca5a13b6b7482fe7acbeeb13f255d\n",
      "  Created: HBM423QJJR545\n",
      "Processing: HBM429.DWKZ.323-a40043a8f837186f5a8827e44389c8d4\n",
      "  Created: HBM429DWKZ323\n",
      "Processing: HBM433.ZLWP.627-5318485b16983482401c3be24b6c42ad\n"
     ]
    }
   ],
   "source": [
    "# Create output root if it doesn't exist\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# Process each directory\n",
    "input_dirs = [d for d in os.listdir(input_root) if os.path.isdir(os.path.join(input_root, d))]\n",
    "\n",
    "for dir_name in input_dirs:\n",
    "    print(f\"Processing: {dir_name}\")\n",
    "    \n",
    "    # Get HubMAP ID\n",
    "    hubmap_id = get_hubmap_id(dir_name)\n",
    "    \n",
    "    # Define paths\n",
    "    input_dir = os.path.join(input_root, dir_name)\n",
    "    json_path = os.path.join(input_dir, 'pipelineConfig.json')\n",
    "    tiff_path = os.path.join(input_dir, 'reg001_expr.ome.tiff')\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = os.path.join(output_root, hubmap_id[:13])  # HBM###.XXXX.###\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create YAML config\n",
    "    yaml_data = create_yaml_config(json_path, tiff_path, hubmap_id)\n",
    "    \n",
    "    # Write YAML file\n",
    "    yaml_path = os.path.join(output_dir, f'{hubmap_id}_config.yaml')\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(yaml_data, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    # Copy TIFF file with new name\n",
    "    new_tiff_path = os.path.join(output_dir, f'{hubmap_id}.ome.tiff')\n",
    "    shutil.copy2(tiff_path, new_tiff_path)\n",
    "    \n",
    "    print(f\"  Created: {hubmap_id}\")\n",
    "\n",
    "print(\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fc378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List created directories\n",
    "output_dirs = sorted([d for d in os.listdir(output_root) if os.path.isdir(os.path.join(output_root, d))])\n",
    "print(f\"Total directories created: {len(output_dirs)}\")\n",
    "print(\"\\nFirst 5 directories:\")\n",
    "for d in output_dirs[:5]:\n",
    "    files = os.listdir(os.path.join(output_root, d))\n",
    "    print(f\"  {d}: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a4548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hra-deepcell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
